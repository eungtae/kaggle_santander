{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import operator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45619, 246) (45619, 1) (929615, 246)\n"
     ]
    }
   ],
   "source": [
    "# 신규 데이터 로딩\n",
    "\n",
    "trn = pd.read_csv('./input/train_append_lb_lag.csv').fillna(0)\n",
    "target = pd.DataFrame(pickle.load(open('./input/target.pkl','rb')), columns=['target'])\n",
    "tst = pd.read_csv('./input/test_append_lb_lag.csv').fillna(0)\n",
    "print(trn.shape, target.shape, tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etc/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/preprocessing/label.py:129: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9452\n",
      "1 1934\n",
      "2 55\n",
      "3 349\n",
      "4 222\n",
      "5 154\n",
      "6 503\n",
      "7 33\n",
      "8 1085\n",
      "9 1219\n",
      "10 246\n",
      "11 21\n",
      "12 2942\n",
      "13 4733\n",
      "14 159\n",
      "15 5151\n",
      "16 8218\n",
      "17 9119\n"
     ]
    }
   ],
   "source": [
    "# 빈도가 낮은 타겟은 사전에 제거 (이유: 교차 검증에 활용할 수 없음 + 너무 빈도가 낮아 무의미함)\n",
    "rem_targets = [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 21, 22, 23]  # 18 classes\n",
    "trn = trn[target['target'].isin(rem_targets)]\n",
    "target = target[target['target'].isin(rem_targets)]\n",
    "target = LabelEncoder().fit_transform(target)\n",
    "\n",
    "for t in np.unique(target):\n",
    "    print(t, sum(target==t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45595, 246) (929615, 246)\n",
      "(45595, 275) (929615, 275)\n"
     ]
    }
   ],
   "source": [
    "cols = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n",
    "        'ind_cder_fin_ult1', 'ind_cno_fin_ult1',  'ind_ctju_fin_ult1',\n",
    "        'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
    "        'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
    "        'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n",
    "        'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
    "        'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n",
    "        'ind_nomina_ult1',   'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
    "\n",
    "print(trn.shape, tst.shape)\n",
    "\n",
    "# 타겟별 누적 합\n",
    "lags = ['_lag_one','_lag_two','_lag_thr','_lag_fou','_lag_fiv']\n",
    "for col in cols:\n",
    "    trn[col+'_sum'] = trn[[col+lag for lag in lags]].sum(axis=1)\n",
    "    tst[col+'_sum'] = tst[[col+lag for lag in lags]].sum(axis=1)\n",
    "    \n",
    "# 월별 누적 합\n",
    "for lag in lags:\n",
    "    trn['sum'+lag] = trn[[col+lag for col in cols]].sum(axis=1)\n",
    "    tst['sum'+lag] = tst[[col+lag for col in cols]].sum(axis=1)\n",
    "    \n",
    "print(trn.shape, tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.92287\teval-mlogloss:1.92563\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.6923\teval-mlogloss:1.69613\n",
      "[2]\ttrain-mlogloss:1.54877\teval-mlogloss:1.55233\n",
      "[3]\ttrain-mlogloss:1.4416\teval-mlogloss:1.4449\n",
      "[4]\ttrain-mlogloss:1.36517\teval-mlogloss:1.36938\n",
      "[5]\ttrain-mlogloss:1.30456\teval-mlogloss:1.30978\n",
      "[6]\ttrain-mlogloss:1.25836\teval-mlogloss:1.26402\n",
      "[7]\ttrain-mlogloss:1.21343\teval-mlogloss:1.21655\n",
      "[8]\ttrain-mlogloss:1.18181\teval-mlogloss:1.18392\n",
      "[9]\ttrain-mlogloss:1.15589\teval-mlogloss:1.15929\n",
      "[10]\ttrain-mlogloss:1.13339\teval-mlogloss:1.13554\n",
      "[11]\ttrain-mlogloss:1.11469\teval-mlogloss:1.11696\n",
      "[12]\ttrain-mlogloss:1.09895\teval-mlogloss:1.10173\n",
      "[13]\ttrain-mlogloss:1.08592\teval-mlogloss:1.0881\n",
      "[14]\ttrain-mlogloss:1.07444\teval-mlogloss:1.07763\n",
      "[15]\ttrain-mlogloss:1.06405\teval-mlogloss:1.06658\n",
      "[16]\ttrain-mlogloss:1.05526\teval-mlogloss:1.05797\n",
      "[17]\ttrain-mlogloss:1.04709\teval-mlogloss:1.05024\n",
      "[18]\ttrain-mlogloss:1.03955\teval-mlogloss:1.04251\n",
      "[19]\ttrain-mlogloss:1.03285\teval-mlogloss:1.03603\n",
      "[20]\ttrain-mlogloss:1.02728\teval-mlogloss:1.03118\n",
      "[21]\ttrain-mlogloss:1.02202\teval-mlogloss:1.02583\n",
      "[22]\ttrain-mlogloss:1.01728\teval-mlogloss:1.02194\n",
      "[23]\ttrain-mlogloss:1.01311\teval-mlogloss:1.01831\n",
      "[24]\ttrain-mlogloss:1.00922\teval-mlogloss:1.01458\n",
      "[25]\ttrain-mlogloss:1.00475\teval-mlogloss:1.01081\n",
      "[26]\ttrain-mlogloss:1.00118\teval-mlogloss:1.00741\n",
      "[27]\ttrain-mlogloss:0.997986\teval-mlogloss:1.00498\n",
      "[28]\ttrain-mlogloss:0.994809\teval-mlogloss:1.00252\n",
      "[29]\ttrain-mlogloss:0.991333\teval-mlogloss:0.999104\n",
      "[30]\ttrain-mlogloss:0.988876\teval-mlogloss:0.99651\n",
      "[31]\ttrain-mlogloss:0.986469\teval-mlogloss:0.994591\n",
      "[32]\ttrain-mlogloss:0.983911\teval-mlogloss:0.992089\n",
      "[33]\ttrain-mlogloss:0.981883\teval-mlogloss:0.990437\n",
      "[34]\ttrain-mlogloss:0.97987\teval-mlogloss:0.988803\n",
      "[35]\ttrain-mlogloss:0.978047\teval-mlogloss:0.987044\n",
      "[36]\ttrain-mlogloss:0.976282\teval-mlogloss:0.985335\n",
      "[37]\ttrain-mlogloss:0.974326\teval-mlogloss:0.98413\n",
      "[38]\ttrain-mlogloss:0.972375\teval-mlogloss:0.982368\n",
      "[39]\ttrain-mlogloss:0.969893\teval-mlogloss:0.980783\n",
      "[40]\ttrain-mlogloss:0.968199\teval-mlogloss:0.979847\n",
      "[41]\ttrain-mlogloss:0.966715\teval-mlogloss:0.978635\n",
      "[42]\ttrain-mlogloss:0.96503\teval-mlogloss:0.977329\n",
      "[43]\ttrain-mlogloss:0.963667\teval-mlogloss:0.97619\n",
      "[44]\ttrain-mlogloss:0.962238\teval-mlogloss:0.974962\n",
      "[45]\ttrain-mlogloss:0.960939\teval-mlogloss:0.974005\n",
      "[46]\ttrain-mlogloss:0.959794\teval-mlogloss:0.973127\n",
      "[47]\ttrain-mlogloss:0.95851\teval-mlogloss:0.972172\n",
      "[48]\ttrain-mlogloss:0.957339\teval-mlogloss:0.971451\n",
      "[49]\ttrain-mlogloss:0.956019\teval-mlogloss:0.970201\n",
      "[50]\ttrain-mlogloss:0.95469\teval-mlogloss:0.969423\n",
      "[51]\ttrain-mlogloss:0.953633\teval-mlogloss:0.968963\n",
      "[52]\ttrain-mlogloss:0.952593\teval-mlogloss:0.967938\n",
      "[53]\ttrain-mlogloss:0.951678\teval-mlogloss:0.96764\n",
      "[54]\ttrain-mlogloss:0.950557\teval-mlogloss:0.966136\n",
      "[55]\ttrain-mlogloss:0.949549\teval-mlogloss:0.965389\n",
      "[56]\ttrain-mlogloss:0.948389\teval-mlogloss:0.964668\n",
      "[57]\ttrain-mlogloss:0.947376\teval-mlogloss:0.964044\n",
      "[58]\ttrain-mlogloss:0.946599\teval-mlogloss:0.963652\n",
      "[59]\ttrain-mlogloss:0.945664\teval-mlogloss:0.963206\n",
      "[60]\ttrain-mlogloss:0.944681\teval-mlogloss:0.962789\n",
      "[61]\ttrain-mlogloss:0.943779\teval-mlogloss:0.96227\n",
      "[62]\ttrain-mlogloss:0.942774\teval-mlogloss:0.961937\n",
      "[63]\ttrain-mlogloss:0.942005\teval-mlogloss:0.961813\n",
      "[64]\ttrain-mlogloss:0.941197\teval-mlogloss:0.961318\n",
      "[65]\ttrain-mlogloss:0.940527\teval-mlogloss:0.961219\n",
      "[66]\ttrain-mlogloss:0.939746\teval-mlogloss:0.960661\n",
      "[67]\ttrain-mlogloss:0.938984\teval-mlogloss:0.960471\n",
      "[68]\ttrain-mlogloss:0.938305\teval-mlogloss:0.960518\n",
      "[69]\ttrain-mlogloss:0.937665\teval-mlogloss:0.960156\n",
      "[70]\ttrain-mlogloss:0.936858\teval-mlogloss:0.960006\n",
      "[71]\ttrain-mlogloss:0.935927\teval-mlogloss:0.959674\n",
      "[72]\ttrain-mlogloss:0.935199\teval-mlogloss:0.959132\n",
      "[73]\ttrain-mlogloss:0.934595\teval-mlogloss:0.958909\n",
      "[74]\ttrain-mlogloss:0.933989\teval-mlogloss:0.958494\n",
      "[75]\ttrain-mlogloss:0.933235\teval-mlogloss:0.958079\n",
      "[76]\ttrain-mlogloss:0.932522\teval-mlogloss:0.957765\n",
      "[77]\ttrain-mlogloss:0.931831\teval-mlogloss:0.95763\n",
      "[78]\ttrain-mlogloss:0.931192\teval-mlogloss:0.957162\n",
      "[79]\ttrain-mlogloss:0.930612\teval-mlogloss:0.957133\n",
      "[80]\ttrain-mlogloss:0.929995\teval-mlogloss:0.956946\n",
      "[81]\ttrain-mlogloss:0.929346\teval-mlogloss:0.956968\n",
      "[82]\ttrain-mlogloss:0.928842\teval-mlogloss:0.956929\n",
      "[83]\ttrain-mlogloss:0.928119\teval-mlogloss:0.956552\n",
      "[84]\ttrain-mlogloss:0.92753\teval-mlogloss:0.956448\n",
      "[85]\ttrain-mlogloss:0.926953\teval-mlogloss:0.956247\n",
      "[86]\ttrain-mlogloss:0.926441\teval-mlogloss:0.956161\n",
      "[87]\ttrain-mlogloss:0.925932\teval-mlogloss:0.956035\n",
      "[88]\ttrain-mlogloss:0.925359\teval-mlogloss:0.955725\n",
      "[89]\ttrain-mlogloss:0.924646\teval-mlogloss:0.955289\n",
      "[90]\ttrain-mlogloss:0.92407\teval-mlogloss:0.954916\n",
      "[91]\ttrain-mlogloss:0.923512\teval-mlogloss:0.954745\n",
      "[92]\ttrain-mlogloss:0.923018\teval-mlogloss:0.954534\n",
      "[93]\ttrain-mlogloss:0.92248\teval-mlogloss:0.95439\n",
      "[94]\ttrain-mlogloss:0.921903\teval-mlogloss:0.954288\n",
      "[95]\ttrain-mlogloss:0.921299\teval-mlogloss:0.954104\n",
      "[96]\ttrain-mlogloss:0.920851\teval-mlogloss:0.954023\n",
      "[97]\ttrain-mlogloss:0.920252\teval-mlogloss:0.953773\n",
      "[98]\ttrain-mlogloss:0.919727\teval-mlogloss:0.953654\n",
      "[99]\ttrain-mlogloss:0.919211\teval-mlogloss:0.953498\n",
      "[100]\ttrain-mlogloss:0.918762\teval-mlogloss:0.953379\n",
      "[101]\ttrain-mlogloss:0.918265\teval-mlogloss:0.953077\n",
      "[102]\ttrain-mlogloss:0.917764\teval-mlogloss:0.95307\n",
      "[103]\ttrain-mlogloss:0.917337\teval-mlogloss:0.952904\n",
      "[104]\ttrain-mlogloss:0.916772\teval-mlogloss:0.952845\n",
      "[105]\ttrain-mlogloss:0.916228\teval-mlogloss:0.952716\n",
      "[106]\ttrain-mlogloss:0.915744\teval-mlogloss:0.952291\n",
      "[107]\ttrain-mlogloss:0.915308\teval-mlogloss:0.952385\n",
      "[108]\ttrain-mlogloss:0.914919\teval-mlogloss:0.952228\n",
      "[109]\ttrain-mlogloss:0.914407\teval-mlogloss:0.95238\n",
      "[110]\ttrain-mlogloss:0.913841\teval-mlogloss:0.952235\n",
      "[111]\ttrain-mlogloss:0.913463\teval-mlogloss:0.952366\n",
      "[112]\ttrain-mlogloss:0.913003\teval-mlogloss:0.952317\n",
      "[113]\ttrain-mlogloss:0.91265\teval-mlogloss:0.952363\n",
      "[114]\ttrain-mlogloss:0.912044\teval-mlogloss:0.952093\n",
      "[115]\ttrain-mlogloss:0.911506\teval-mlogloss:0.952031\n",
      "[116]\ttrain-mlogloss:0.910983\teval-mlogloss:0.951898\n",
      "[117]\ttrain-mlogloss:0.910524\teval-mlogloss:0.951876\n",
      "[118]\ttrain-mlogloss:0.909984\teval-mlogloss:0.951795\n",
      "[119]\ttrain-mlogloss:0.909512\teval-mlogloss:0.95168\n",
      "[120]\ttrain-mlogloss:0.909127\teval-mlogloss:0.951688\n",
      "[121]\ttrain-mlogloss:0.908765\teval-mlogloss:0.951726\n",
      "[122]\ttrain-mlogloss:0.908359\teval-mlogloss:0.951838\n",
      "[123]\ttrain-mlogloss:0.907894\teval-mlogloss:0.951983\n",
      "[124]\ttrain-mlogloss:0.907473\teval-mlogloss:0.951938\n",
      "[125]\ttrain-mlogloss:0.907068\teval-mlogloss:0.951917\n",
      "[126]\ttrain-mlogloss:0.9066\teval-mlogloss:0.952015\n",
      "[127]\ttrain-mlogloss:0.906218\teval-mlogloss:0.951944\n",
      "[128]\ttrain-mlogloss:0.905827\teval-mlogloss:0.951823\n",
      "[129]\ttrain-mlogloss:0.90535\teval-mlogloss:0.951908\n",
      "Stopping. Best iteration:\n",
      "[119]\ttrain-mlogloss:0.909512\teval-mlogloss:0.95168\n",
      "\n",
      "[0]\ttrain-mlogloss:2.04497\teval-mlogloss:2.03869\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.75855\teval-mlogloss:1.75464\n",
      "[2]\ttrain-mlogloss:1.58686\teval-mlogloss:1.58393\n",
      "[3]\ttrain-mlogloss:1.46797\teval-mlogloss:1.46515\n",
      "[4]\ttrain-mlogloss:1.38224\teval-mlogloss:1.38165\n",
      "[5]\ttrain-mlogloss:1.31575\teval-mlogloss:1.31571\n",
      "[6]\ttrain-mlogloss:1.26616\teval-mlogloss:1.26695\n",
      "[7]\ttrain-mlogloss:1.21798\teval-mlogloss:1.21681\n",
      "[8]\ttrain-mlogloss:1.18496\teval-mlogloss:1.18455\n",
      "[9]\ttrain-mlogloss:1.15777\teval-mlogloss:1.15852\n",
      "[10]\ttrain-mlogloss:1.13451\teval-mlogloss:1.13487\n",
      "[11]\ttrain-mlogloss:1.11605\teval-mlogloss:1.11644\n",
      "[12]\ttrain-mlogloss:1.09857\teval-mlogloss:1.09953\n",
      "[13]\ttrain-mlogloss:1.08496\teval-mlogloss:1.08639\n",
      "[14]\ttrain-mlogloss:1.0733\teval-mlogloss:1.07497\n",
      "[15]\ttrain-mlogloss:1.06355\teval-mlogloss:1.06566\n",
      "[16]\ttrain-mlogloss:1.05385\teval-mlogloss:1.05623\n",
      "[17]\ttrain-mlogloss:1.04588\teval-mlogloss:1.0485\n",
      "[18]\ttrain-mlogloss:1.03912\teval-mlogloss:1.04233\n",
      "[19]\ttrain-mlogloss:1.03198\teval-mlogloss:1.03662\n",
      "[20]\ttrain-mlogloss:1.02627\teval-mlogloss:1.03154\n",
      "[21]\ttrain-mlogloss:1.02089\teval-mlogloss:1.02605\n",
      "[22]\ttrain-mlogloss:1.01615\teval-mlogloss:1.0218\n",
      "[23]\ttrain-mlogloss:1.01126\teval-mlogloss:1.01771\n",
      "[24]\ttrain-mlogloss:1.00695\teval-mlogloss:1.0139\n",
      "[25]\ttrain-mlogloss:1.00297\teval-mlogloss:1.01155\n",
      "[26]\ttrain-mlogloss:0.999424\teval-mlogloss:1.0089\n",
      "[27]\ttrain-mlogloss:0.996556\teval-mlogloss:1.0061\n",
      "[28]\ttrain-mlogloss:0.993061\teval-mlogloss:1.00361\n",
      "[29]\ttrain-mlogloss:0.990292\teval-mlogloss:1.00178\n",
      "[30]\ttrain-mlogloss:0.987729\teval-mlogloss:0.999539\n",
      "[31]\ttrain-mlogloss:0.985088\teval-mlogloss:0.997574\n",
      "[32]\ttrain-mlogloss:0.982526\teval-mlogloss:0.996072\n",
      "[33]\ttrain-mlogloss:0.980271\teval-mlogloss:0.994511\n",
      "[34]\ttrain-mlogloss:0.978325\teval-mlogloss:0.99319\n",
      "[35]\ttrain-mlogloss:0.976297\teval-mlogloss:0.99153\n",
      "[36]\ttrain-mlogloss:0.974398\teval-mlogloss:0.990081\n",
      "[37]\ttrain-mlogloss:0.972701\teval-mlogloss:0.989066\n",
      "[38]\ttrain-mlogloss:0.971152\teval-mlogloss:0.988287\n",
      "[39]\ttrain-mlogloss:0.969317\teval-mlogloss:0.987044\n",
      "[40]\ttrain-mlogloss:0.967371\teval-mlogloss:0.98541\n",
      "[41]\ttrain-mlogloss:0.966056\teval-mlogloss:0.984442\n",
      "[42]\ttrain-mlogloss:0.964511\teval-mlogloss:0.983361\n",
      "[43]\ttrain-mlogloss:0.9631\teval-mlogloss:0.982366\n",
      "[44]\ttrain-mlogloss:0.961582\teval-mlogloss:0.981262\n",
      "[45]\ttrain-mlogloss:0.960455\teval-mlogloss:0.98079\n",
      "[46]\ttrain-mlogloss:0.959114\teval-mlogloss:0.98026\n",
      "[47]\ttrain-mlogloss:0.957966\teval-mlogloss:0.979749\n",
      "[48]\ttrain-mlogloss:0.956627\teval-mlogloss:0.978459\n",
      "[49]\ttrain-mlogloss:0.955168\teval-mlogloss:0.977798\n",
      "[50]\ttrain-mlogloss:0.953301\teval-mlogloss:0.976562\n",
      "[51]\ttrain-mlogloss:0.952227\teval-mlogloss:0.975934\n",
      "[52]\ttrain-mlogloss:0.951287\teval-mlogloss:0.975456\n",
      "[53]\ttrain-mlogloss:0.950224\teval-mlogloss:0.975124\n",
      "[54]\ttrain-mlogloss:0.949021\teval-mlogloss:0.97468\n",
      "[55]\ttrain-mlogloss:0.948215\teval-mlogloss:0.974268\n",
      "[56]\ttrain-mlogloss:0.947181\teval-mlogloss:0.974174\n",
      "[57]\ttrain-mlogloss:0.946174\teval-mlogloss:0.97336\n",
      "[58]\ttrain-mlogloss:0.945227\teval-mlogloss:0.972968\n",
      "[59]\ttrain-mlogloss:0.944281\teval-mlogloss:0.972605\n",
      "[60]\ttrain-mlogloss:0.943358\teval-mlogloss:0.972408\n",
      "[61]\ttrain-mlogloss:0.942448\teval-mlogloss:0.971962\n",
      "[62]\ttrain-mlogloss:0.941524\teval-mlogloss:0.971522\n",
      "[63]\ttrain-mlogloss:0.94092\teval-mlogloss:0.971276\n",
      "[64]\ttrain-mlogloss:0.940033\teval-mlogloss:0.970862\n",
      "[65]\ttrain-mlogloss:0.939065\teval-mlogloss:0.970412\n",
      "[66]\ttrain-mlogloss:0.938414\teval-mlogloss:0.970352\n",
      "[67]\ttrain-mlogloss:0.9376\teval-mlogloss:0.970376\n",
      "[68]\ttrain-mlogloss:0.936926\teval-mlogloss:0.970213\n",
      "[69]\ttrain-mlogloss:0.935933\teval-mlogloss:0.969909\n",
      "[70]\ttrain-mlogloss:0.935191\teval-mlogloss:0.96968\n",
      "[71]\ttrain-mlogloss:0.934365\teval-mlogloss:0.969305\n",
      "[72]\ttrain-mlogloss:0.933498\teval-mlogloss:0.969007\n",
      "[73]\ttrain-mlogloss:0.932876\teval-mlogloss:0.969131\n",
      "[74]\ttrain-mlogloss:0.932205\teval-mlogloss:0.969037\n",
      "[75]\ttrain-mlogloss:0.931658\teval-mlogloss:0.968982\n",
      "[76]\ttrain-mlogloss:0.931065\teval-mlogloss:0.968778\n",
      "[77]\ttrain-mlogloss:0.930572\teval-mlogloss:0.968711\n",
      "[78]\ttrain-mlogloss:0.92985\teval-mlogloss:0.968775\n",
      "[79]\ttrain-mlogloss:0.929204\teval-mlogloss:0.968625\n",
      "[80]\ttrain-mlogloss:0.928653\teval-mlogloss:0.96855\n",
      "[81]\ttrain-mlogloss:0.928114\teval-mlogloss:0.968358\n",
      "[82]\ttrain-mlogloss:0.927569\teval-mlogloss:0.968245\n",
      "[83]\ttrain-mlogloss:0.926902\teval-mlogloss:0.968043\n",
      "[84]\ttrain-mlogloss:0.926161\teval-mlogloss:0.967809\n",
      "[85]\ttrain-mlogloss:0.925528\teval-mlogloss:0.96746\n",
      "[86]\ttrain-mlogloss:0.924895\teval-mlogloss:0.967651\n",
      "[87]\ttrain-mlogloss:0.924361\teval-mlogloss:0.967566\n",
      "[88]\ttrain-mlogloss:0.923727\teval-mlogloss:0.967293\n",
      "[89]\ttrain-mlogloss:0.923104\teval-mlogloss:0.967465\n",
      "[90]\ttrain-mlogloss:0.922637\teval-mlogloss:0.967632\n",
      "[91]\ttrain-mlogloss:0.922072\teval-mlogloss:0.967678\n",
      "[92]\ttrain-mlogloss:0.921606\teval-mlogloss:0.967651\n",
      "[93]\ttrain-mlogloss:0.921097\teval-mlogloss:0.967495\n",
      "[94]\ttrain-mlogloss:0.920551\teval-mlogloss:0.96761\n",
      "[95]\ttrain-mlogloss:0.919961\teval-mlogloss:0.967549\n",
      "[96]\ttrain-mlogloss:0.919423\teval-mlogloss:0.967506\n",
      "[97]\ttrain-mlogloss:0.918836\teval-mlogloss:0.967312\n",
      "[98]\ttrain-mlogloss:0.918308\teval-mlogloss:0.967323\n",
      "Stopping. Best iteration:\n",
      "[88]\ttrain-mlogloss:0.923727\teval-mlogloss:0.967293\n",
      "\n",
      "[0]\ttrain-mlogloss:2.04432\teval-mlogloss:2.05018\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.75852\teval-mlogloss:1.76284\n",
      "[2]\ttrain-mlogloss:1.58662\teval-mlogloss:1.58984\n",
      "[3]\ttrain-mlogloss:1.46743\teval-mlogloss:1.4703\n",
      "[4]\ttrain-mlogloss:1.3819\teval-mlogloss:1.38504\n",
      "[5]\ttrain-mlogloss:1.31571\teval-mlogloss:1.31848\n",
      "[6]\ttrain-mlogloss:1.26603\teval-mlogloss:1.26874\n",
      "[7]\ttrain-mlogloss:1.21714\teval-mlogloss:1.22245\n",
      "[8]\ttrain-mlogloss:1.18383\teval-mlogloss:1.18935\n",
      "[9]\ttrain-mlogloss:1.15668\teval-mlogloss:1.16264\n",
      "[10]\ttrain-mlogloss:1.13332\teval-mlogloss:1.14061\n",
      "[11]\ttrain-mlogloss:1.11447\teval-mlogloss:1.12181\n",
      "[12]\ttrain-mlogloss:1.09799\teval-mlogloss:1.10576\n",
      "[13]\ttrain-mlogloss:1.08457\teval-mlogloss:1.09261\n",
      "[14]\ttrain-mlogloss:1.07265\teval-mlogloss:1.08156\n",
      "[15]\ttrain-mlogloss:1.06243\teval-mlogloss:1.07179\n",
      "[16]\ttrain-mlogloss:1.0529\teval-mlogloss:1.06343\n",
      "[17]\ttrain-mlogloss:1.04505\teval-mlogloss:1.05583\n",
      "[18]\ttrain-mlogloss:1.03791\teval-mlogloss:1.04925\n",
      "[19]\ttrain-mlogloss:1.03162\teval-mlogloss:1.0433\n",
      "[20]\ttrain-mlogloss:1.026\teval-mlogloss:1.03829\n",
      "[21]\ttrain-mlogloss:1.02106\teval-mlogloss:1.03387\n",
      "[22]\ttrain-mlogloss:1.01639\teval-mlogloss:1.02956\n",
      "[23]\ttrain-mlogloss:1.01131\teval-mlogloss:1.02503\n",
      "[24]\ttrain-mlogloss:1.00758\teval-mlogloss:1.02199\n",
      "[25]\ttrain-mlogloss:1.00375\teval-mlogloss:1.019\n",
      "[26]\ttrain-mlogloss:1.00022\teval-mlogloss:1.01599\n",
      "[27]\ttrain-mlogloss:0.997192\teval-mlogloss:1.01365\n",
      "[28]\ttrain-mlogloss:0.994475\teval-mlogloss:1.0115\n",
      "[29]\ttrain-mlogloss:0.991137\teval-mlogloss:1.00857\n",
      "[30]\ttrain-mlogloss:0.988352\teval-mlogloss:1.00623\n",
      "[31]\ttrain-mlogloss:0.985504\teval-mlogloss:1.0036\n",
      "[32]\ttrain-mlogloss:0.983331\teval-mlogloss:1.00184\n",
      "[33]\ttrain-mlogloss:0.980817\teval-mlogloss:1.00021\n",
      "[34]\ttrain-mlogloss:0.978612\teval-mlogloss:0.998622\n",
      "[35]\ttrain-mlogloss:0.976727\teval-mlogloss:0.997049\n",
      "[36]\ttrain-mlogloss:0.974373\teval-mlogloss:0.995185\n",
      "[37]\ttrain-mlogloss:0.97269\teval-mlogloss:0.994048\n",
      "[38]\ttrain-mlogloss:0.971022\teval-mlogloss:0.992908\n",
      "[39]\ttrain-mlogloss:0.969291\teval-mlogloss:0.99167\n",
      "[40]\ttrain-mlogloss:0.967426\teval-mlogloss:0.990366\n",
      "[41]\ttrain-mlogloss:0.96584\teval-mlogloss:0.989195\n",
      "[42]\ttrain-mlogloss:0.964385\teval-mlogloss:0.988281\n",
      "[43]\ttrain-mlogloss:0.962883\teval-mlogloss:0.987348\n",
      "[44]\ttrain-mlogloss:0.961447\teval-mlogloss:0.9864\n",
      "[45]\ttrain-mlogloss:0.96027\teval-mlogloss:0.985736\n",
      "[46]\ttrain-mlogloss:0.958914\teval-mlogloss:0.98483\n",
      "[47]\ttrain-mlogloss:0.957692\teval-mlogloss:0.984145\n",
      "[48]\ttrain-mlogloss:0.956464\teval-mlogloss:0.983211\n",
      "[49]\ttrain-mlogloss:0.955327\teval-mlogloss:0.982385\n",
      "[50]\ttrain-mlogloss:0.954114\teval-mlogloss:0.981899\n",
      "[51]\ttrain-mlogloss:0.952955\teval-mlogloss:0.981217\n",
      "[52]\ttrain-mlogloss:0.951712\teval-mlogloss:0.980456\n",
      "[53]\ttrain-mlogloss:0.950777\teval-mlogloss:0.980063\n",
      "[54]\ttrain-mlogloss:0.949813\teval-mlogloss:0.979731\n",
      "[55]\ttrain-mlogloss:0.948795\teval-mlogloss:0.979087\n",
      "[56]\ttrain-mlogloss:0.947765\teval-mlogloss:0.978509\n",
      "[57]\ttrain-mlogloss:0.946421\teval-mlogloss:0.977505\n",
      "[58]\ttrain-mlogloss:0.945649\teval-mlogloss:0.976975\n",
      "[59]\ttrain-mlogloss:0.944807\teval-mlogloss:0.976568\n",
      "[60]\ttrain-mlogloss:0.943785\teval-mlogloss:0.976251\n",
      "[61]\ttrain-mlogloss:0.942894\teval-mlogloss:0.975537\n",
      "[62]\ttrain-mlogloss:0.942076\teval-mlogloss:0.97519\n",
      "[63]\ttrain-mlogloss:0.941322\teval-mlogloss:0.974994\n",
      "[64]\ttrain-mlogloss:0.940497\teval-mlogloss:0.974783\n",
      "[65]\ttrain-mlogloss:0.939766\teval-mlogloss:0.974428\n",
      "[66]\ttrain-mlogloss:0.939015\teval-mlogloss:0.974044\n",
      "[67]\ttrain-mlogloss:0.938042\teval-mlogloss:0.973611\n",
      "[68]\ttrain-mlogloss:0.937273\teval-mlogloss:0.973346\n",
      "[69]\ttrain-mlogloss:0.936511\teval-mlogloss:0.972887\n",
      "[70]\ttrain-mlogloss:0.935855\teval-mlogloss:0.972929\n",
      "[71]\ttrain-mlogloss:0.93527\teval-mlogloss:0.972716\n",
      "[72]\ttrain-mlogloss:0.934556\teval-mlogloss:0.972849\n",
      "[73]\ttrain-mlogloss:0.933885\teval-mlogloss:0.972487\n",
      "[74]\ttrain-mlogloss:0.933184\teval-mlogloss:0.972242\n",
      "[75]\ttrain-mlogloss:0.932519\teval-mlogloss:0.971997\n",
      "[76]\ttrain-mlogloss:0.93185\teval-mlogloss:0.971971\n",
      "[77]\ttrain-mlogloss:0.931175\teval-mlogloss:0.971642\n",
      "[78]\ttrain-mlogloss:0.930484\teval-mlogloss:0.971204\n",
      "[79]\ttrain-mlogloss:0.929932\teval-mlogloss:0.971105\n",
      "[80]\ttrain-mlogloss:0.929311\teval-mlogloss:0.970975\n",
      "[81]\ttrain-mlogloss:0.928604\teval-mlogloss:0.970465\n",
      "[82]\ttrain-mlogloss:0.927914\teval-mlogloss:0.970099\n",
      "[83]\ttrain-mlogloss:0.927376\teval-mlogloss:0.969975\n",
      "[84]\ttrain-mlogloss:0.926839\teval-mlogloss:0.969802\n",
      "[85]\ttrain-mlogloss:0.926316\teval-mlogloss:0.969733\n",
      "[86]\ttrain-mlogloss:0.925676\teval-mlogloss:0.969715\n",
      "[87]\ttrain-mlogloss:0.92493\teval-mlogloss:0.96915\n",
      "[88]\ttrain-mlogloss:0.924062\teval-mlogloss:0.969056\n",
      "[89]\ttrain-mlogloss:0.923571\teval-mlogloss:0.969181\n",
      "[90]\ttrain-mlogloss:0.92313\teval-mlogloss:0.969094\n",
      "[91]\ttrain-mlogloss:0.922511\teval-mlogloss:0.968837\n",
      "[92]\ttrain-mlogloss:0.921984\teval-mlogloss:0.968662\n",
      "[93]\ttrain-mlogloss:0.92106\teval-mlogloss:0.96826\n",
      "[94]\ttrain-mlogloss:0.920465\teval-mlogloss:0.968168\n",
      "[95]\ttrain-mlogloss:0.919933\teval-mlogloss:0.968182\n",
      "[96]\ttrain-mlogloss:0.919531\teval-mlogloss:0.9682\n",
      "[97]\ttrain-mlogloss:0.918967\teval-mlogloss:0.968205\n",
      "[98]\ttrain-mlogloss:0.91849\teval-mlogloss:0.968081\n",
      "[99]\ttrain-mlogloss:0.918011\teval-mlogloss:0.968014\n",
      "[100]\ttrain-mlogloss:0.917397\teval-mlogloss:0.967945\n",
      "[101]\ttrain-mlogloss:0.916904\teval-mlogloss:0.967827\n",
      "[102]\ttrain-mlogloss:0.916378\teval-mlogloss:0.967706\n",
      "[103]\ttrain-mlogloss:0.915927\teval-mlogloss:0.967685\n",
      "[104]\ttrain-mlogloss:0.915441\teval-mlogloss:0.967566\n",
      "[105]\ttrain-mlogloss:0.915003\teval-mlogloss:0.967497\n",
      "[106]\ttrain-mlogloss:0.914448\teval-mlogloss:0.967342\n",
      "[107]\ttrain-mlogloss:0.914028\teval-mlogloss:0.967181\n",
      "[108]\ttrain-mlogloss:0.913558\teval-mlogloss:0.967178\n",
      "[109]\ttrain-mlogloss:0.913099\teval-mlogloss:0.966953\n",
      "[110]\ttrain-mlogloss:0.91262\teval-mlogloss:0.966859\n",
      "[111]\ttrain-mlogloss:0.91216\teval-mlogloss:0.966901\n",
      "[112]\ttrain-mlogloss:0.911693\teval-mlogloss:0.967108\n",
      "[113]\ttrain-mlogloss:0.911267\teval-mlogloss:0.967003\n",
      "[114]\ttrain-mlogloss:0.91081\teval-mlogloss:0.966909\n",
      "[115]\ttrain-mlogloss:0.9104\teval-mlogloss:0.967026\n",
      "[116]\ttrain-mlogloss:0.909898\teval-mlogloss:0.966894\n",
      "[117]\ttrain-mlogloss:0.909502\teval-mlogloss:0.966968\n",
      "[118]\ttrain-mlogloss:0.909042\teval-mlogloss:0.966806\n",
      "[119]\ttrain-mlogloss:0.908642\teval-mlogloss:0.966885\n",
      "[120]\ttrain-mlogloss:0.908241\teval-mlogloss:0.967072\n",
      "[121]\ttrain-mlogloss:0.90773\teval-mlogloss:0.967234\n",
      "[122]\ttrain-mlogloss:0.907355\teval-mlogloss:0.967282\n",
      "[123]\ttrain-mlogloss:0.906957\teval-mlogloss:0.967139\n",
      "[124]\ttrain-mlogloss:0.906444\teval-mlogloss:0.966902\n",
      "[125]\ttrain-mlogloss:0.906028\teval-mlogloss:0.967038\n",
      "[126]\ttrain-mlogloss:0.905631\teval-mlogloss:0.966914\n",
      "[127]\ttrain-mlogloss:0.905038\teval-mlogloss:0.966746\n",
      "[128]\ttrain-mlogloss:0.904548\teval-mlogloss:0.966754\n",
      "[129]\ttrain-mlogloss:0.904157\teval-mlogloss:0.966808\n",
      "[130]\ttrain-mlogloss:0.903758\teval-mlogloss:0.967023\n",
      "[131]\ttrain-mlogloss:0.903408\teval-mlogloss:0.96703\n",
      "[132]\ttrain-mlogloss:0.902927\teval-mlogloss:0.966922\n",
      "[133]\ttrain-mlogloss:0.902579\teval-mlogloss:0.96693\n",
      "[134]\ttrain-mlogloss:0.902156\teval-mlogloss:0.967004\n",
      "[135]\ttrain-mlogloss:0.901674\teval-mlogloss:0.966741\n",
      "[136]\ttrain-mlogloss:0.901267\teval-mlogloss:0.966713\n",
      "[137]\ttrain-mlogloss:0.9008\teval-mlogloss:0.966618\n",
      "[138]\ttrain-mlogloss:0.900306\teval-mlogloss:0.966771\n",
      "[139]\ttrain-mlogloss:0.899919\teval-mlogloss:0.966864\n",
      "[140]\ttrain-mlogloss:0.89943\teval-mlogloss:0.966811\n",
      "[141]\ttrain-mlogloss:0.899124\teval-mlogloss:0.966932\n",
      "[142]\ttrain-mlogloss:0.898792\teval-mlogloss:0.966953\n",
      "[143]\ttrain-mlogloss:0.898367\teval-mlogloss:0.966824\n",
      "[144]\ttrain-mlogloss:0.897983\teval-mlogloss:0.966648\n",
      "[145]\ttrain-mlogloss:0.897547\teval-mlogloss:0.966815\n",
      "[146]\ttrain-mlogloss:0.897138\teval-mlogloss:0.966844\n",
      "[147]\ttrain-mlogloss:0.896764\teval-mlogloss:0.967167\n",
      "Stopping. Best iteration:\n",
      "[137]\ttrain-mlogloss:0.9008\teval-mlogloss:0.966618\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'log loss': [0.90535043611532529, 0.91830840965971972, 0.89676387589100792]},\n",
       " {'log loss': [0.95190776783483833, 0.96732326325262985, 0.96716711426461666]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGB Model Param\n",
    "num_round = 500\n",
    "early_stop = 10\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    \n",
    "    # 모델 복잡도\n",
    "    'max_depth': 2, # 높을 수록 복잡\n",
    "    #'gamma': 3,    # 낮을 수록 복잡\n",
    "    #'min_child_weight': 5, # 낮을 수록 복잡\n",
    "\n",
    "    # 랜덤 샘플링을 통한 정규화\n",
    "    #'colsample_bylevel': 0.7,\n",
    "    #'colsample_bytree': 1,\n",
    "    #'subsample': 0.8,\n",
    "\n",
    "    # 정규화\n",
    "    #'reg_alpha': 2,\n",
    "    #'reg_lambda': 3,\n",
    "\n",
    "    # 학습 속도\n",
    "    #'learning_rate': 0.03,\n",
    "    \n",
    "    # 기본 설정\n",
    "    'nthread': 4,\n",
    "    'num_class': 18,\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 777,\n",
    "}\n",
    "\n",
    "def evaluate_xgb(x, y):\n",
    "    trn_scores = dict(); vld_scores = dict()\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=777)\n",
    "    for t_ind, v_ind in sss.split(x,y):\n",
    "        # split data\n",
    "        x_trn, x_vld = x.iloc[t_ind], x.iloc[v_ind]\n",
    "        y_trn, y_vld = y[t_ind], y[v_ind]\n",
    "\n",
    "        dtrn = xgb.DMatrix(x_trn, label=y_trn)\n",
    "        dvld = xgb.DMatrix(x_vld, label=y_vld)\n",
    "        watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "\n",
    "        # fit xgb\n",
    "        bst = xgb.train(xgb_params, dtrn, num_round, watch_list, \\\n",
    "                        early_stopping_rounds=early_stop, verbose_eval=True)\n",
    "        \n",
    "        # eval _ trn        \n",
    "        preds = bst.predict(dtrn)\n",
    "\n",
    "        log_scores = trn_scores.get('log loss', [])\n",
    "        log_scores.append(log_loss(y_trn, preds))\n",
    "        trn_scores['log loss'] = log_scores\n",
    "\n",
    "        # eval _ vld\n",
    "        preds = bst.predict(dvld)\n",
    "        \n",
    "        log_scores = vld_scores.get('log loss', [])\n",
    "        log_scores.append(log_loss(y_vld, preds))\n",
    "        vld_scores['log loss'] = log_scores\n",
    "    return trn_scores, vld_scores\n",
    "\n",
    "def print_scores(trn_scores, vld_scores):\n",
    "    prefix = '        '\n",
    "    cols = ['log loss']\n",
    "    print('='*50)\n",
    "    print('TRAIN EVAL')\n",
    "    for col in cols:\n",
    "        print('-'*50)\n",
    "        print('# {}'.format(col))\n",
    "        print('# {} Mean : {}'.format(prefix, np.mean(trn_scores[col])))\n",
    "        print('# {} Raw  : {}'.format(prefix, trn_scores[col]))\n",
    "\n",
    "    print('='*50)\n",
    "    print('VALID EVAL')\n",
    "    for col in cols:\n",
    "        print('-'*50)\n",
    "        print('# {}'.format(col))\n",
    "        print('# {} Mean : {}'.format(prefix, np.mean(vld_scores[col])))\n",
    "        print('# {} Raw  : {}'.format(prefix, vld_scores[col]))\n",
    "\n",
    "def print_time(end, start):\n",
    "    print('='*50)\n",
    "    elapsed = end - start\n",
    "    print('{} secs'.format(round(elapsed)))\n",
    "    \n",
    "def fit_and_eval(trn, target, model):\n",
    "    trn_scores, vld_scores = evaluate(trn,target,model)\n",
    "    print_scores(trn_scores, vld_scores)\n",
    "    print_time(time.time(), st)    \n",
    "\n",
    "evaluate_xgb(trn,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# Test shape : (929615, 275)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 기반 결과물 생성 코드\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print('='*50)\n",
    "print('# Test shape : {}'.format(tst.shape))\n",
    "\n",
    "# 최종 모델 정의 및 학습 실행\n",
    "dtrn = xgb.DMatrix(trn, label= target)\n",
    "num_round = 115 # 평가 함수 기반 최적의 num_round 수치 지정\n",
    "bst = xgb.train(xgb_params, dtrn, num_round, verbose_eval=True)\n",
    "\n",
    "dtst = xgb.DMatrix(tst)\n",
    "preds = bst.predict(dtst)\n",
    "preds = np.fliplr(np.argsort(preds, axis=1))\n",
    "\n",
    "cols = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n",
    "        'ind_cder_fin_ult1', 'ind_cno_fin_ult1',  'ind_ctju_fin_ult1',\n",
    "        'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
    "        'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
    "        'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n",
    "        'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
    "        'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n",
    "        'ind_nomina_ult1',   'ind_nom_pens_ult1', 'ind_recibo_ult1']\n",
    "target_cols = [cols[i] for i, col in enumerate(cols) if i in rem_targets]\n",
    "\n",
    "final_preds = []\n",
    "for pred in preds:\n",
    "    top_products = []\n",
    "    for i, product in enumerate(pred):\n",
    "        top_products.append(target_cols[product])\n",
    "        if i == 6:\n",
    "            break\n",
    "    final_preds.append(' '.join(top_products))\n",
    "\n",
    "temp = pd.read_csv('./input/test_clean.csv')\n",
    "test_id = temp['ncodpers']\n",
    "out_df = pd.DataFrame({'ncodpers':test_id, 'added_products':final_preds})\n",
    "file_name = datetime.now().strftime(\"result_%Y%m%d%H%M%S\") + '.csv'\n",
    "out_df.to_csv(os.path.join('./output',file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
